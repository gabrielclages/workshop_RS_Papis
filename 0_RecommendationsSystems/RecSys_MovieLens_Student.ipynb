{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems: Collaborative Filtering\n",
    "\n",
    "## Introduction\n",
    "\n",
    "First of all, we should start with a definition of the problem. A recommender system is a software that exploits user’s preferences to suggests items to users. It helps users to find what they are looking for and it helps users to discover new interesting never seen items.\n",
    "\n",
    "Deep Learning is acquiring great notoriety nowadays because of its ability to solve complex tasks such as image recognition, natural language processing, speech recognition, and more.\n",
    "\n",
    "Companies like Netflix, Facebook and Google are using Deep Learning in order to determine what is more appealing to their users; and keep their engagement at a high.\n",
    "\n",
    "Two different approaches may be adopted to solve the recommendation problem:\n",
    "\n",
    "- Content Based\n",
    "- Collaborative Filtering\n",
    "\n",
    "The former exploits item’s description to infer a rate, the latter exploits user’s neighborhood so it’s based on the concept that similar users give similar rate to items. In this tutorial we will cover the Collaborative Filtering approach.\n",
    "\n",
    "Collaborative filtering (CF) models aim to exploit information\n",
    "about users’ preferences for items (e.g. star ratings)\n",
    "to provide personalised recommendations. Owing to the\n",
    "Netflix challenge, a panoply of different CF models have\n",
    "been proposed, with popular choices being matrix factorisation and neighbourhood models. This first tutorial on Recommendation Systems follows AutoRec, a CF model based on the autoencoder\n",
    "paradigm; our interest in this paradigm stems from\n",
    "the successes of (deep) neural network models for vision\n",
    "and speech tasks. In this tutorial we implement a symple AutoRec system.\n",
    "\n",
    "<img src=\"images/recsys0.png\">\n",
    "\n",
    "This is a shallow neural net with only one hidden layer <font color='red'>(##TODO: maybe you can change that as part of an exercise)</font>. So we’ll just feed in all the movie ratings watched by a user and expect a more generalized rating distribution per user to come out. We can use this approach to obtain an idea of what their ratings would be for movies they haven’t watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to build a recommender system using TensorFlow. We’ll use other useful packages such as:\n",
    "\n",
    "- NumPy\n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Matplotlib\n",
    "\n",
    "Let's import all of them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "import tensorflow as tf\n",
    "# Importing some more libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will be using the MovieLens 1M dataset which contains 1 million ratings from 6000 users on 4000 movies. Ratings are contained in the file “ratings.dat” in the following format:\n",
    "\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "We also are presented with \"movies.dat\" in the following format:\n",
    "\n",
    "MovieID::Movie::Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to split our dataset in training set and test set. The training set is used to train our model and the test set will be used only to evaluate the learned model. We split the dataset using the Hold-Out 80/20 protocol, so 80% of ratings for each user are kept in the training set, the remaining 20% will be moved to the test set. If you have a dataset with few ratings, the best choice for splitting protocol would be the K-Fold. For the rows which have no data we should fill it with 0. Could you do that? \n",
    "\n",
    "- <font color='red'>(##TODO: Please complete the dataset for the missing values with 0.)</font> \n",
    "\n",
    "\n",
    "\n",
    "- <font color='red'>(##TODO: Divide the training and testing dataset with the Hold-out protocol described above. ##HINT: You may find this <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\">this</a> helpful.)</font>\n",
    "\n",
    "Let’s load the dataset with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ratings data\n",
    "ratings = pd.read_csv('ratings.dat',\\\n",
    "          sep=\"::\", header = None, engine='python')\n",
    "movies = pd.read_csv('movies.dat',\\\n",
    "          sep=\"::\", header = None, engine='python')\n",
    "# Pivotting the data to get it at a user level\n",
    "ratings_pivot = pd.pivot_table(ratings[[0,1,2]],\\\n",
    "          values=2, index=0, columns=1 ).fillna(0)\n",
    "# YOUR CODE HERE\n",
    "# ratings = (...)\n",
    "# Creating train and test sets\n",
    "# YOUR CODE HERE\n",
    "# X_train, X_test = train_test_split(ratings, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the top 5 rows of the ratings dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for movies:\n",
    "\n",
    "- <font color='red'>(##TODO: Let's use the same method as the example before to see the first 5 entries in the movie dataframe)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many movies there are in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture\n",
    "\n",
    "Now, we have the data at user level, with their ratings as the features. Yay! There are a total of 3706 movies. So our dataset had 3706 features per user. Let's start building the model! Discover how I know there is 3706 movies in the model?\n",
    "- <font color='red'>(##TODO: Complete with code to determine the number of movies)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_movies = ##YOUR CODE HERE\n",
    "\n",
    "# Deciding how many nodes each layer should have\n",
    "n_nodes_inpl = number_movies \n",
    "n_nodes_hl1  = 256  \n",
    "n_nodes_outl = number_movies \n",
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_inpl+1,n_nodes_hl1]))}\n",
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "output_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1+1,n_nodes_outl]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define how many nodes each layer has and what the weight matrix will looke like. For this part, we are replicating the image shown at the beginning of the tutorial. Here we are not defining any bias matrix associated with the layers. This is due to the fact that we are going to add a bias node to each one of the layers which has a constant value of one.\n",
    "\n",
    "- <font color='red'>(##TODO: Fill the bias with a constant value of one. ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/fill\">this</a> helpful.)</font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.placeholder('float', [None, number_movies])\n",
    "# Add a constant node to the first layer\n",
    "input_layer_const = # YOUR CODE\n",
    "input_layer_concat =  tf.concat([input_layer, input_layer_const], 1)\n",
    "# Multiply output of input_layer wth a weight matrix \n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat,\\\n",
    "hidden_1_layer_vals['weights']))\n",
    "# Adding one bias node to the hidden layer\n",
    "layer1_const = # YOUR CODE\n",
    "layer_concat =  tf.concat([layer_1, layer1_const], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='red'>(##TODO:Multiply output of hidden with a weight matrix to get final output. ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/matmul\">this</a> helpful.)</font> \n",
    "- <font color='red'> Brownie points for you if you can tell me at the beginning of the next session in a consise way why we would use reduce_mean as opposed to reduce_sum </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply output of hidden with a weight matrix to get final output\n",
    "output_layer = ## YOUR CODE\n",
    "# Output_true shall have the original shape for error calculations\n",
    "output_true = tf.placeholder('float', [None, number_movies])\n",
    "# Define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# Define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Model is build. Now, let's determine how many epochs the model runs. We also need to start the session!\n",
    "- <font color='red'>##TODO:How many images do we have on the dataset? (##HINT: You may find this <a href=\"https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.shape.html\">this</a> helpful.)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# Defining batch size, number of epochs and learning rate\n",
    "batch_size = 100\n",
    "number_epochs = 20 \n",
    "total_images = ##YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, off to training. We are running the model for a 20 epochs taking 100 users in batches. \n",
    "<font color='red'>(##TODO:You can increase the number of epochs and see how your model does? Want to try?)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_epoch_list = []\n",
    "for epoch in range(number_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "    \n",
    "    for i in range(int(total_images/batch_size)):\n",
    "        epoch_x = X_train[ i*batch_size : (i+1)*batch_size ]\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "        \n",
    "    output_train = sess.run(output_layer,\\\n",
    "               feed_dict={input_layer:X_train})\n",
    "    output_test = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:X_test})\n",
    "        \n",
    "    print('MSE train', MSE(output_train, X_train),'MSE test', MSE(output_test, X_test))      \n",
    "    print('Epoch', epoch, '/', number_epochs, 'loss:',epoch_loss)\n",
    "    loss_epoch_list.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>(##TODO:Now can you plot the `loss_epoch_list`?)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put this model into use. Let's get the predicted rating for an user. For that, all you have to do is pass it through the net once. If you are going to use the output to recommend that user some movies.\n",
    "<font color='red'>##TODO:Can you do that? (##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/Session\">this</a> helpful.)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a user\n",
    "sample_user = X_test.iloc[95,:]\n",
    "#get the predicted ratings\n",
    "sample_user_pred = ## YOUR CODE\n",
    "sample_user_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References / Credit\n",
    "\n",
    "[0] http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf\n",
    "\n",
    "[1] https://medium.com/@connectwithghosh/recommender-system-on-the-movielens-using-an-autoencoder-using-tensorflow-in-python-f13d3e8d600d\n",
    "\n",
    "[2] http://www.vitobellini.com/posts/2018/01/03/how-to-build-a-recommender-system-in-tensorflow.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
