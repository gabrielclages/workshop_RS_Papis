{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "First of all, we should start with a definition of the problem. A recommender system is a software that exploits user’s preferences to suggests items to users. It helps users to find what they are looking for and it helps users to discover new interesting never seen items.\n",
    "\n",
    "Deep Learning is acquiring a great notoriety nowadays because of the leverage of huge computational power and its capacity to solve complex tasks such as image recognition, natural language processing, speech recognition, and more.\n",
    "\n",
    "Companies like Netflix, Facebook and Google are using Deep Learning in order to determine what is more appealing to their users; and keep their engagement at a high.\n",
    "\n",
    "Two different approaches may be adopted to solve the recommendation problem:\n",
    "\n",
    "- Content Based\n",
    "- Collaborative Filtering\n",
    "\n",
    "The former exploits item’s description to infer a rate, the latter exploits user’s neighborhood so it’s based on the concept that similar users give similar rate to items. In this tutorial we will cover the Collaborative Filtering approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to build a recommender system using TensorFlow. We’ll use other useful packages such as:\n",
    "\n",
    "- NumPy\n",
    "- Pandas\n",
    "- Sklearn\n",
    "\n",
    "Let's import all of them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will be using the MovieLens 1M dataset which contains 1 million ratings from 6000 users on 4000 movies. Ratings are contained in the file “ratings.dat” in the following format:\n",
    "\n",
    "UserID::MovieID::Rating::Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to split our dataset in training set and test set. The training set is used to train our model and the test set will be used only to evaluate the learned model. We split the dataset using the Hold-Out 80/20 protocol, so 80% of ratings for each user are kept in the training set, the remaining 20% will be moved to the test set. If you have a dataset with few ratings, the best choice for splitting protocol would be the K-Fold.\n",
    "\n",
    "Let’s load the dataset with pandas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 6040 ITEMS: 3677\n"
     ]
    }
   ],
   "source": [
    "# Reading dataset\n",
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t', names=['user', 'item', 'rating', 'timestamp'], header=None)\n",
    "df = df.drop('timestamp', axis=1)\n",
    "\n",
    "num_items = df.item.nunique()\n",
    "num_users = df.user.nunique()\n",
    "\n",
    "print(\"USERS: {} ITEMS: {}\".format(num_users, num_items))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will load training set into a DataFrame with three columns: user, item and ratings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize in [0, 1]\n",
    "\n",
    "r = df['rating'].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "df['rating'] = df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames cannot be directly used to feed a model, the best option is to convert a DataFrame into a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame in user-item matrix\n",
    "\n",
    "matrix = df.pivot(index='user', columns='item', values='rating')\n",
    "matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows in matrix will correspond to users and columns to items, therefore entries correspond to ratings given by users to an items. Our matrix is still an object of DataFrame type, we need to convert it to a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Users and items ordered as they are in matrix\n",
    "\n",
    "users = matrix.index.tolist()\n",
    "items = matrix.columns.tolist()\n",
    "\n",
    "matrix = matrix.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can start to setup some network parameters, such as the dimension of each hidden layer, in this tutorial we will use 2 hidden layers.\n",
    "\n",
    "X is a placeholder, it just tells to TensorFlow that we have a variable X in the computational graph.\n",
    "\n",
    "Weights and biases are dictionaries of variables, randomly initialized of type float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_input = num_items\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now define our model. Autoencoders are unsupervised learning neural networks, they try to reconstruct input data at the output, this means that they learn a compressed representation of the input, and they use that to reconstruct the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "\n",
    "# Prediction\n",
    "\n",
    "y_pred = decoder_op\n",
    "\n",
    "\n",
    "# Targets are the input data.\n",
    "\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the structure of neural network has been defined, we need a loss function. A loss function quantifies how much bad is our estimate on the current example, using the current parameters W for the model. The cost function is just the average of the loss function over all the examples in the training set. Said that, we want to minimize our loss. Different optimizer can be used such as Adam, Adagrad, Adadelta and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# Define evaluation metrics\n",
    "\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of TensorFlow uses computational graph for his operations, placeholders and variables must be initialized, at this point no more variables can be allocated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally start to train our model.\n",
    "\n",
    "We split training data into batches and we feed the network with them. Using mini-batch technique is useful to speed up the training because weights are updated one time per batch. You may also think to shuffle your mini-batches to make the gradient more variable, hence it can help to converge because increases the likelihood of hitting a good direction and prevents some local minima.\n",
    "\n",
    "We train our model with vectors of user ratings, each vector represents a user and each column an item. As previously said, entries are ratings that the user gave to items. The main idea is to encode input data in a smaller space, a meaningful representation for users based on their rating, to predict unrated items.\n",
    "\n",
    "Let’s back to the code, we are going to train our model for 100 epochs with a batch size of 250. This means that the entire training set will feed our neural network 100 times, every time using 250 users.\n",
    "\n",
    "At the end of training, encoder will contains a compact representation of the input data. We will then use the decoder to reconstruct the original user rating but this time we will have a score even for unrated user’s items based on the learned representation for other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.358672621349\n",
      "Epoch: 2 Loss: 0.358332136025\n",
      "Epoch: 3 Loss: 0.357110819469\n",
      "Epoch: 4 Loss: 0.352607370665\n",
      "Epoch: 5 Loss: 0.335546404123\n",
      "Epoch: 6 Loss: 0.294640678912\n",
      "Epoch: 7 Loss: 0.263646443064\n",
      "Epoch: 8 Loss: 0.202023894216\n",
      "Epoch: 9 Loss: 0.0762476248201\n",
      "Epoch: 10 Loss: 0.0225673589545\n",
      "Epoch: 11 Loss: 0.0192257810074\n",
      "Epoch: 12 Loss: 0.0180094168754\n",
      "Epoch: 13 Loss: 0.0172605379485\n",
      "Epoch: 14 Loss: 0.0167130654057\n",
      "Epoch: 15 Loss: 0.0164310803715\n",
      "Epoch: 16 Loss: 0.0161783787577\n",
      "Epoch: 17 Loss: 0.0161252878218\n",
      "Epoch: 18 Loss: 0.0160263289387\n",
      "Epoch: 19 Loss: 0.0157362773316\n",
      "Epoch: 20 Loss: 0.0156403116416\n",
      "Predictions...\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    epochs = 20\n",
    "    batch_size = 250\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(matrix.shape[0] / batch_size)\n",
    "    matrix = np.array_split(matrix, num_batches)\n",
    "    \n",
    "    avg_cost_list = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "\n",
    "        for batch in matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "        \n",
    "        avg_cost_list.append(avg_cost)\n",
    "\n",
    "        print(\"Epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    print(\"Predictions...\")\n",
    "\n",
    "    matrix = np.concatenate(matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: matrix})\n",
    "\n",
    "    predictions = predictions.append(pd.DataFrame(preds))\n",
    "\n",
    "    predictions = predictions.stack().reset_index(name='rating')\n",
    "    predictions.columns = ['user', 'item', 'rating']\n",
    "    predictions['user'] = predictions['user'].map(lambda value: users[value])\n",
    "    predictions['item'] = predictions['item'].map(lambda value: items[value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the loss can be plotted. This will allow to see how much are model is \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_cost_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to evaluate our model, but first user’s ratings in the training set must be removed. We keep only the top-10 ranked items for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now able to build a recommender system with the same performances of other Collaborative Filtering algorithms such as Matrix Factorization.\n",
    "\n",
    "You can play with network settings such as hidden layers’ dimension as see how system’s performances change. Generally, their dimension depends on the complexity of the function you want to approximate. If your hidden layers are too big, you may experience overfitting and your model will lose the capacity to generalize well on test set. On the contrary, if they are too small, the neural network would not have enough parameters to fit well the data. You may also want to improve performances trying some regularization techniques like dropout. Could you be able to do that? Add the dropout layer if you have time. <font color='green'>##HINT: You can make use of the tf.layers.dropout</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
