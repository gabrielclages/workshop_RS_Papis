{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems: Deep and Wide\n",
    "\n",
    "All Rights Reserved to Google.\n",
    "\n",
    "<img src=\"https://www.balabit.com/wp-content/uploads/2017/02/collaborative_filtering4-e1438083772522.png\"  width=\"400\">\n",
    "\n",
    "In the previous session we learned about Collaborative Filtering. We also had somewhat easier exercises left to the audience. In this session, we will have more intense exercises.\n",
    "\n",
    "The wide and deep tensorflow model was developed at Google, where they combined linear model and deep neural network. This architecture helped Google to achieve better product recommendations for Play Store users. They idea for this tutorial stems from a various famous <a href=\"https://ai.google/research/pubs/pub45413\" style=\"color: #6D00FF;\">paper</a> that has been making its rounds in the industry.\n",
    "\n",
    "In this notebook we present some basics of the most up-to-date recommender systems to really get you started in this field. \n",
    "\n",
    "<img src=\"images/wide_n_deep.png\"  width=\"1000\">\n",
    "\n",
    "The figure above shows a comparison of a wide model (logistic regression with sparse features and transformations), a deep model (feed-forward neural network with an embedding layer and several hidden layers), and a Wide & Deep model (joint training of both). \n",
    "\n",
    "### GOALS: \n",
    "\n",
    "In this tutorial we will learn about:\n",
    "- Wide models, Deep models and Wide and Deep models.\n",
    "- Exporting Models to Prediction\n",
    "- Running Predictions\n",
    "\n",
    "\n",
    "Ready?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Making the necessary imports\n",
    "\n",
    "This tutorial will be making use of mostly tensorflow, pandas and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the data\n",
    "\n",
    "These steps are to organize the csv features in two different categories: numerical and categorical. These is a crucial step and the building blocks for the models we will be building thoughout this block.\n",
    "\n",
    "\n",
    "Let's assume I told you that the following features are:\n",
    "    \n",
    "- categorical: workclass, education, marital_status, occupation, relationship, race, gender, native_country, income_bracker\n",
    "\n",
    "- numerical: age, fnlwgt, education_num, occupation, capital_loss, hours_per_week, native_country\n",
    "\n",
    "<font color='red'>\n",
    "##TODO: How would you construct the `_CSV_COLUMN_STRUCTURE` seen as categorical features are represented by `[''] `and numerical by `[0]`? \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "_CSV_COLUMN_STRUCTURE = ##YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature columns\n",
    "\n",
    "The dataset used in this tutorial is used to predict the probability that the individual has an annual income of over 50,000 dollars using the Census Income Dataset. This is a common issue in Recommender systems. Although this problem could be solved using Logistic Regression; we can make use of a more advanced model in order to make more accurate predictions. The data can be found at `census_data/` and will be loaded later. Let's define the base categorical and continuous feature columns that we'll use. These base columns will be the building blocks used by both the wide part and the deep part of the model.\n",
    "#### Wide Columns\n",
    "The wide model is a linear model with a wide set of sparse and crossed feature columns. Wide models with crossed feature columns can memorize sparse interactions between features effectively. That being said, one limitation of crossed feature columns is that they do not generalize to feature combinations that have not appeared in the training data. \n",
    "\n",
    "\n",
    "- <font color='red'>(##TODO: How would you be able to define the categorial columns? ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list\">this</a> helpful.)</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous columns\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "education_num = tf.feature_column.numeric_column('education_num')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "education = ##CODE HERE(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "marital_status = ##CODE HERE(\n",
    "    'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "relationship = ##CODE HERE(\n",
    "    'relationship', [\n",
    "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "        'Other-relative'])\n",
    "\n",
    "workclass = ##CODE HERE(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='red'>(##TODO: Let's assume that for occupation your sparse features are in string format, and you want to distribute your inputs into a finite number of buckets by hashing. How can you do this? Let's assume the hash_bucket size is 1000 ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket\">this</a> helpful.)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing:\n",
    " occupation = ##YOUR CODE HERE\n",
    "\n",
    " # Transformations.\n",
    " age_buckets = tf.feature_column.bucketized_column(\n",
    "     age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    " # Wide columns is a combination of base_columns and crossed\n",
    " base_columns = [\n",
    "     education, marital_status, relationship, workclass, occupation,\n",
    "     age_buckets,\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='red'>(From what was taught in the class part, how can we define the wide columns?</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        ['education', 'occupation'], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),\n",
    "]\n",
    "\n",
    "## wide_columns = ##YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Columns\n",
    "The deep model is a feed-forward neural network. Each of the sparse, high-dimensional categorical features are first converted into a low-dimensional and dense real-valued vector, often referred to as an embedding vector. These low-dimensional dense embedding vectors are concatenated with the continuous features, and then fed into the hidden layers of a neural network in the forward pass. The embedding values are initialized randomly, and are trained along with all other model parameters to minimize the training loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  deep_columns = [\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "      tf.feature_column.indicator_column(workclass),\n",
    "      tf.feature_column.indicator_column(education),\n",
    "      tf.feature_column.indicator_column(marital_status),\n",
    "      tf.feature_column.indicator_column(relationship),\n",
    "      tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Function\n",
    "In order to create our model we will be making use of Tensorflow's <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\" style=\"color: #6D00FF;\">tf.estimator API</a>. For that we need to define an input function. Here we need to parse the input data and then use the Dataset API in order to represent an input pipeline as a collection of elements (nested structures of tensors) and a \"logical plan\" of transformations that act on those elements.\n",
    "\n",
    "\n",
    "<font color='red'>##TODO: Complete the code in such a way that you can parse the record into tensors and repear the input indefinetely, and the batch. ##HINT: You may find this <a href=\"https://www.tensorflow.org/programmers_guide/datasets\">this</a> helpful.</font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "\n",
    "  def parse_csv(value):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_STRUCTURE)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('income_bracket')\n",
    "    return features, tf.equal(labels, '>50K')\n",
    "\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
    "    \n",
    "  ## YOUR CODE HERE\n",
    "\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "Now, on to the \"fun\" part: modeling. For this part we will create 3 different models: wide, deep and wide-and-deep and compare the results from each one.\n",
    "\n",
    "<font color='red'>##TODO: You can play with the `hidden_units` to see how that affects the loss of your model. This will only affect deep models, obviously. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [100, 75, 50, 25]\n",
    "train_epochs = 40\n",
    "epochs_per_eval = 2\n",
    "batch_size = 40\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "\n",
    "run_config = tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "train_data = 'census_data/adult.data'\n",
    "test_data = 'census_data/adult.test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Wide Model\n",
    "\n",
    "The wide model is a linear model with a wide set of sparse and crossed feature columns. Wide models with crossed feature columns can memorize sparse interactions between features effectively. That being said, one limitation of crossed feature columns is that they do not generalize to feature combinations that have not appeared in the training data. \n",
    "\n",
    "<font color='red'>##TODO: You can create the wide model in multiple ways, however, the easiest way is by using a canned trainer. If you have already done that, I would encourage you to think how you would build it from scratch. Call the instructor if you want to run by ideas.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'wide_model'\n",
    "#model =  ##YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate. <font color='red'>##TODO: Add code to evaluate your trainer. ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate\">this</a> helpful.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Train and evaluate the model every epochs_per_eval epochs.\n",
    "  for n in range(train_epochs // epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        train_data, epochs_per_eval, True, batch_size))\n",
    "\n",
    "    results = ##YOUR CODE HERE\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print('Results at epoch', (n + 1) * epochs_per_eval)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "      print('%s: %s' % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Deep Model\n",
    "\n",
    "The deep model is a feed-forward neural network, as shown in the previous figure. Each of the sparse, high-dimensional categorical features are first converted into a low-dimensional and dense real-valued vector, often referred to as an embedding vector. These low-dimensional dense embedding vectors are concatenated with the continuous features, and then fed into the hidden layers of a neural network in the forward pass. The embedding values are initialized randomly, and are trained along with all other model parameters to minimize the training loss. We will actually learn more about embeddings in the last session of today's tutorial.\n",
    "\n",
    "Another way to represent categorical columns to feed into a neural network is via a one-hot or multi-hot representation. This is often appropriate for categorical columns with only a few possible values. As an example of a one-hot representation, for the relationship column, \"Husband\" can be represented as [1, 0, 0, 0, 0, 0], and \"Not-in-family\" as [0, 1, 0, 0, 0, 0], etc. This is a fixed representation, whereas embeddings are more flexible and calculated at training time.\n",
    "\n",
    "We'll configure the embeddings for the categorical columns using embedding_column, and concatenate them with the continuous columns. We also use indicator_column to create multi-hot representations of some categorical columns.\n",
    "\n",
    "<font color='red'>##TODO: You can create the deep model in multiple ways, however, the easiest way is by using a canned trainer. If you have already done that, I would encourage you to think how you would build it from scratch. Call the instructor if you want to run by ideas.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'deep_model'\n",
    "#model =  ##YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate. <font color='red'>##TODO: Add code to evaluate your trainer. ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate\">this</a> helpful.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Train and evaluate the model every epochs_per_eval epochs.\n",
    "  for n in range(train_epochs // epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        train_data, epochs_per_eval, True, batch_size))\n",
    "\n",
    "    results = ##YOUR CODE HERE\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print('Results at epoch', (n + 1) * epochs_per_eval)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "      print('%s: %s' % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Wide and Deep Model\n",
    "<font color='red'>##TODO: You can create the wide and deep model in multiple ways, however, the easiest way is by using a canned trainer `DNNLinearCombinedClassifier`. If you have already done that, I would encourage you to think how you would build it from scratch. Call the instructor if you want to run by ideas.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'wide_and_deep_model'\n",
    "# model = ##YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate. <font color='red'>##TODO: Add code to evaluate your trainer. ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate\">this</a> helpful.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Train and evaluate the model every epochs_per_eval epochs.\n",
    "  for n in range(train_epochs // epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        train_data, epochs_per_eval, True, batch_size))\n",
    "\n",
    "    results = ##YOUR CODE HERE\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print('Results at epoch', (n + 1) * epochs_per_eval)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "      print('%s: %s' % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exporting your model\n",
    "<font color='red'>##TODO: Now export your model. ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel\">this</a> helpful.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ## YOUR CODE\n",
    "  # servable_model_path = ## YOUR CODE\n",
    "  print(\"*********** Done Exporting at Path - %s\", servable_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Feature contains Lists which may hold zero or more values.  These lists are the base values BytesList, FloatList, Int64List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "\treturn tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "\treturn tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "\treturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Predictions\n",
    "\n",
    "Now we are ready to use the saved model to make predictions. Isn't that exciting? You have built not one, not two, but three models in this section. And will be using the Estimator API to export our saved model.\n",
    "\n",
    "\n",
    "<font color='red'>##TODO:Use pandas to read the input csv.  ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel\">this</a> helpful.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionoutputfile = 'census_output.csv'\n",
    "predictioninputfile = 'census_input.csv'\n",
    "\n",
    "input_file = ## YOUR CODE HERE\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>##TODO:Load from saved model and get the predictor.  ##HINT: You may find this <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/predictor/from_saved_model\">this</a> helpful.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    ##YOUR CODE HERE:\n",
    "    # LOAD SAVED MODEL\n",
    "    # GET PREDICTOR\n",
    "    #predictor = \n",
    "\n",
    "    prediction_OutFile = open(predictionoutputfile, 'w')\n",
    "\n",
    "    # Write Header for CSV file\n",
    "    prediction_OutFile.write(\"age, workclass, fnlwgt, education, education_num,marital_status, occupation, relationship, race, gender,capital_gain, capital_loss, hours_per_week, native_country,predicted_income_bracket,probability\")\n",
    "    prediction_OutFile.write('\\n')\n",
    "\n",
    "    # Read file and create feature_dict for each record\n",
    "    with open(predictioninputfile) as inf:\n",
    "        # Skip header\n",
    "        next(inf)\n",
    "        for line in inf:\n",
    "\n",
    "            # Read data, using python, into our features\n",
    "            age, workclass, fnlwgt, education, education_num, marital_status, occupation, relationship, race, gender, capital_gain, capital_loss, hours_per_week, native_country = line.strip().split(\",\")\n",
    "\n",
    "            # Create a feature_dict for train.example - Get Feature Columns using\n",
    "            feature_dict = {\n",
    "                'age': _float_feature(value=int(age)),\n",
    "                'workclass': _bytes_feature(value=workclass.encode()),\n",
    "                'fnlwgt': _float_feature(value=int(fnlwgt)),\n",
    "                'education': _bytes_feature(value=education.encode()),\n",
    "                'education_num': _float_feature(value=int(education_num)),\n",
    "                'marital_status': _bytes_feature(value=marital_status.encode()),\n",
    "                'occupation': _bytes_feature(value=occupation.encode()),\n",
    "                'relationship': _bytes_feature(value=relationship.encode()),\n",
    "                'race': _bytes_feature(value=race.encode()),\n",
    "                'gender': _bytes_feature(value=gender.encode()),\n",
    "                'capital_gain': _float_feature(value=int(capital_gain)),\n",
    "                'capital_loss': _float_feature(value=int(capital_loss)),\n",
    "                'hours_per_week': _float_feature(value=float(hours_per_week)),\n",
    "                'native_country': _bytes_feature(value=native_country.encode()),\n",
    "            }\n",
    "\n",
    "            # Prepare model input\n",
    "\n",
    "            model_input = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "\n",
    "            model_input = model_input.SerializeToString()\n",
    "            output_dict = predictor({\"inputs\": [model_input]})\n",
    "\n",
    "            print(\" prediction Label is \", output_dict['classes'])\n",
    "            print('Probability : ' + str(output_dict['scores']))\n",
    "\n",
    "            prediction_OutFile.write(str(age)+ \",\" + workclass+ \",\" + str(fnlwgt)+ \",\" + education+ \",\" + str(education_num) + \",\" + marital_status + \",\" + occupation + \",\" + relationship + \",\" + race+ \",\" +gender+ \",\" + str(capital_gain)+ \",\" + str(capital_loss)+ \",\" + str(hours_per_week)+ \",\" + native_country+ \",\")\n",
    "            label_index = np.argmax(output_dict['scores'])\n",
    "            prediction_OutFile.write(str(label_index))\n",
    "            prediction_OutFile.write(',')\n",
    "            prediction_OutFile.write(str(output_dict['scores'][0][label_index]))\n",
    "            prediction_OutFile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have finished this session. You may want to iterate through the model until obtaining a smaller eval loss. Some things you can change:\n",
    "\n",
    "- Batch Size\n",
    "- Number of Epochs\n",
    "\n",
    "Can you try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits / Source:\n",
    "\n",
    "[0] https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "\n",
    "[1] https://ai.google/research/pubs/pub45413"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
